{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "model_selection_log_reg.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/WilliamYkZhang/COMP551_A2/blob/master/model_selection_log_reg.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "idSn_CEXemcP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Preprocessing\n",
        "from sklearn.preprocessing import Normalizer\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.feature_selection import SelectKBest\n",
        "from sklearn.pipeline import Pipeline\n",
        "from nltk.corpus import stopwords\n",
        "import pandas as pd\n",
        "\n",
        "# Transformers \n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer \n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "\n",
        "# Models \n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC \n",
        "import xgboost as xgb\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "# Module to write final params \n",
        "import csv\n",
        "import datetime\n",
        "import pickle "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l0v-GpOleu3Z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "b3618819-eaff-4dcd-c8b5-b6159de55e85"
      },
      "source": [
        "# Download stopwords\n",
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_iF-zaZewkN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get a list of stopwords\n",
        "stopwords = stopwords.words(\"english\")\n",
        "\n",
        "# Transformers \n",
        "c_vect = CountVectorizer(lowercase=True, encoding=\"utf-8\", decode_error=\"ignore\", strip_accents='unicode',stop_words=stopwords, analyzer = \"word\")\n",
        "tfidf_vect = TfidfVectorizer(lowercase=True, encoding = \"utf-8\",  decode_error = 'ignore', strip_accents='unicode', stop_words=stopwords, analyzer = \"word\")  \n",
        "tfidf_trans = TfidfTransformer()\n",
        "svd = TruncatedSVD()\n",
        "nml = Normalizer()\n",
        "\n",
        "# Estimators \n",
        "log_reg = LogisticRegression()\n",
        "svc = SVC() # class weight , experiement values \n",
        "xgb_clf = xgb.XGBClassifier(objective='multi:softmax')\n",
        "decision_tree_clf = DecisionTreeClassifier()\n",
        "rff = RandomForestClassifier()\n",
        "multi_NB = MultinomialNB()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xSeypWaje14b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Building pipeline \n",
        "pipeline_tfidf = Pipeline([('tfidf', tfidf_vect), ('clf', xgb_clf)], verbose=True)\n",
        "\n",
        "# Instantiate parameters for pipeline     \n",
        "parameters_tfidf = {\n",
        "    'tfidf__max_features': (None, 10000, 25000, 50000),\n",
        "    'tfidf__use_idf': (True, False), # Enable inverse-document-frequency reweighting.\n",
        "    'tfidf__max_df': (0.5, 0.75, 0.9), # ignore terms that have a document frequency strictly higher than the given threshold\n",
        "    'tfidf__min_df': (0.025, 0.05, 0.1), #  ignore terms that have a document frequency strictly lower than the given threshold\n",
        "    'tfidf__norm': ('l1', 'l2', None), # regularization term\n",
        "    'tfidf__smooth_idf': (True, False), # Smooth idf weights by adding one to document frequencies, as if an extra document was seen containing every term in the collection exactly once.Prevents zero divisions\n",
        "    'tfidf__ngram_range': ((1, 1), (1, 2)), # n-grams to be extracted     \n",
        "}  "
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}